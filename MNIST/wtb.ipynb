{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well tempered backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "ROOT='/home/ansuini/repos/WellTemperedSGD/MNIST'\n",
    "RES=join(ROOT,'results')\n",
    "datum='data_shuffled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tensors(model,verbose=False):\n",
    "    '''\n",
    "    Init to zero a list of tensors with the same shapes of model.parameters()\n",
    "    '''\n",
    "    tensors = [torch.zeros_like(p) for p in model.parameters()]     \n",
    "    \n",
    "    if verbose:\n",
    "        print('Tensors shapes:')\n",
    "        _ = [print(t.shape) for t in tensors]\n",
    "    \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tensors_one(model,verbose=False):\n",
    "    '''\n",
    "    Init to one a list of tensors with the same shapes of model.parameters()\n",
    "    '''\n",
    "    tensors = [torch.ones_like(p) for p in model.parameters()]     \n",
    "    \n",
    "    if verbose:\n",
    "        print('Tensors shapes:')\n",
    "        _ = [print(t.shape) for t in tensors]\n",
    "    \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_grad(grad, model):\n",
    "    '''\n",
    "    Accumulate grad in a list of tensors \n",
    "    of the same structure of model.parameters() \n",
    "    '''\n",
    "    for g, p in zip(grad, model.parameters()):\n",
    "        g += p.grad\n",
    "    return grad\n",
    "        \n",
    "def acc_grad2(grad2, model):\n",
    "    '''\n",
    "    Accumulate squared grad in a list of tensors \n",
    "    of the same structure of model.parameters() \n",
    "    '''\n",
    "    for g, p in zip(grad2, model.parameters()):\n",
    "        g += torch.mul(p.grad, p.grad)\n",
    "    return grad2\n",
    "\n",
    "def clone_tensors(tensors):\n",
    "    '''\n",
    "    Clone gradient data to make some tests\n",
    "    '''\n",
    "    return [t.grad.clone() for t in tensors]\n",
    "\n",
    "def compute_snr(grad, grad2, B, device):\n",
    "    '''\n",
    "    Compute snr\n",
    "    '''  \n",
    "    \n",
    "    epsilon = 1e-8 #small quantity to be added to err in order to avoid division by zero\n",
    "    \n",
    "    snr = [] #list of tensors with the same structure as model.parameters()\n",
    "    \n",
    "    for g, g2 in zip(grad, grad2):\n",
    "        \n",
    "        # work with clones in order to avoid modifications of the original data in this function\n",
    "        g_copy  = g.clone()\n",
    "        g2_copy = g2.clone()\n",
    "    \n",
    "        # average over number of batches (B is the same as in the paper)\n",
    "        g_copy = g_copy/B   \n",
    "        g2_copy = g2_copy/B  \n",
    "        \n",
    "        # compute error    \n",
    "        assert(torch.sum(g2_copy - g_copy*g_copy >= 0) ) # assert if the variance is non-negative\n",
    "        \n",
    "        err = torch.sqrt( ( g2_copy - g_copy*g_copy )/ B ) # the error is the square root of the variance divided by B\n",
    "        err[err==0] = epsilon # add small positive quantity if err is 0\n",
    "        \n",
    "        # compute signal to error ratio\n",
    "        # snr is the ratio between the abs value of the gradient averaged\n",
    "        # over B batches and the err\n",
    "        snr.append(torch.div( torch.abs(g_copy), err ) ) \n",
    "            \n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # dropout is 0 by default\n",
    "    def __init__(self,p1=0.0, p2=0.0):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.dropout1 = nn.Dropout(p=p1)\n",
    "        self.dropout2 = nn.Dropout(p=p2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(model, loader, device):    \n",
    "    model.eval()    \n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(loader):\n",
    "                \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)            \n",
    "            loss += F.nll_loss(output, target)*data.shape[0]\n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                \n",
    "    loss /= len(loader.dataset)\n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "\n",
    "    if loader.dataset.train==True:\n",
    "        datatype='training'\n",
    "    else:\n",
    "        datatype='test'\n",
    "        \n",
    "    print(datatype + ' set: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "           loss, correct, len(loader.dataset), acc))\n",
    "    \n",
    "    return loss.item(),acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, device):   \n",
    "    # init tensors to store gradients    \n",
    "    grad = init_tensors(model)\n",
    "    grad2 = init_tensors(model)\n",
    "    \n",
    "    model.train()\n",
    "    B = 0 # count mini-batches\n",
    "    \n",
    "    # iterate on 1 epoch accumulating grad and grad2\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # control the number of training samples\n",
    "        if batch_idx*train_loader.batch_size > nsamples_train:\n",
    "            break\n",
    "        \n",
    "        B += 1\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        grad = acc_grad(grad,model)\n",
    "        grad2 = acc_grad2(grad2,model)\n",
    "    \n",
    "    print('N.of minibatches: {}'.format(B) )\n",
    "    return grad, grad2, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=6000\n",
    "nsamples_train=60000\n",
    "momentum=0.0\n",
    "epochs=3\n",
    "lr=0.01\n",
    "seed=1101\n",
    "WTB = True # if False use normal backpropagation\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# Please notice that shuffle is False here in the training_loader. \n",
    "# This is essential if we want to restrict the training dataset\n",
    "# to nsamples_training < 60000. It is not essential to set it to \n",
    "# False in the test_loader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(join(ROOT,datum), train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(join(ROOT,datum), train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 20, 24, 24]             520\n",
      "            Conv2d-2             [-1, 50, 8, 8]          25,050\n",
      "           Dropout-3                  [-1, 800]               0\n",
      "            Linear-4                  [-1, 500]         400,500\n",
      "           Dropout-5                  [-1, 500]               0\n",
      "            Linear-6                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 1.64\n",
      "Estimated Total Size (MB): 1.77\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "print(summary(model,(1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well tempered backprop!\n",
      "\n",
      "Epoch: 1\n",
      "training set: average loss: 2.2810, accuracy: 10020/60000 (17%)\n",
      "test set: average loss: 2.2791, accuracy: 1717/10000 (17%)\n",
      "\n",
      "Epoch: 2\n",
      "training set: average loss: 2.2514, accuracy: 16647/60000 (28%)\n",
      "test set: average loss: 2.2490, accuracy: 2920/10000 (29%)\n",
      "\n",
      "Epoch: 3\n",
      "training set: average loss: 2.2190, accuracy: 24978/60000 (42%)\n",
      "test set: average loss: 2.2161, accuracy: 4297/10000 (43%)\n"
     ]
    }
   ],
   "source": [
    "if WTB:\n",
    "    print('Well tempered backprop!')\n",
    "else:\n",
    "    print('Normal backprop!')\n",
    "\n",
    "# training\n",
    "train_stats = []\n",
    "test_stats = []\n",
    "    \n",
    "# init snr to 1 the first time\n",
    "snr = init_tensors_one(model)\n",
    "    \n",
    "# iterate over epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('\\nEpoch: {}'.format(epoch))      \n",
    "    # ----------------------------- train    \n",
    "    # init tensors to store gradients    \n",
    "    grad = init_tensors(model)\n",
    "    grad2 = init_tensors(model)\n",
    "    \n",
    "    model.train()\n",
    "    B = 0 # count mini-batches  \n",
    "    # iterate on 1 epoch accumulating grad and grad2\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # control the number of training samples\n",
    "        if batch_idx*train_loader.batch_size > nsamples_train:\n",
    "            break        \n",
    "        B += 1        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        grad = acc_grad(grad,model)\n",
    "        grad2 = acc_grad2(grad2,model)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # gradient modification and update\n",
    "            for p,s in zip(model.parameters(),snr): \n",
    "                \n",
    "                # modify grad with snr computed on the previous epoch\n",
    "                p.grad = torch.where( s < 1, s*p.grad, p.grad) \n",
    "                \n",
    "                # update parameters with the new gradient\n",
    "                p.data -= lr*p.grad.data\n",
    "    \n",
    "    # if WTB compute snr, otherwise it will remain 1 and will not affect backprop\n",
    "    if WTB:\n",
    "        # update snr at the end of the epoch\n",
    "        with torch.no_grad():\n",
    "            snr = compute_snr(grad, grad2, B, device)\n",
    "        \n",
    "                                          \n",
    "    train_stats.append(stats(model, train_loader, device))\n",
    "    test_stats.append(stats(model, test_loader, device))\n",
    "\n",
    "    \n",
    "if SAVE:\n",
    "    if WTB:\n",
    "        np.save(join(RES, 'train_stats_wtb'), train_stats)\n",
    "        np.save(join(RES, 'test_stats_wtb'), test_stats)\n",
    "    else:\n",
    "        np.save(join(RES, 'train_stats_norm'), train_stats)\n",
    "        np.save(join(RES, 'test_stats_norm'), test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_norm = np.load(join(RES,'test_stats_norm.npy'))\n",
    "train_stats_norm = np.load(join(RES,'train_stats_norm.npy'))\n",
    "test_stats_wtb = np.load(join(RES,'test_stats_wtb.npy'))\n",
    "train_stats_wtb = np.load(join(RES,'train_stats_wtb.npy'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
