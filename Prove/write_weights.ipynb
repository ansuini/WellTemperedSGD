{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA fully-connected ReLU network with one hidden layer, trained to predict y from x\\nby minimizing squared Euclidean distance.\\nThis implementation uses the nn package from PyTorch to build the network.\\nPyTorch autograd makes it easy to define computational graphs and take gradients,\\nbut raw autograd can be a bit too low-level for defining complex neural networks;\\nthis is where the nn package can help. The nn package defines a set of Modules,\\nwhich you can think of as a neural network layer that has produces output from\\ninput and may have some trainable weights or other state.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing squared Euclidean distance.\n",
    "This implementation uses the nn package from PyTorch to build the network.\n",
    "PyTorch autograd makes it easy to define computational graphs and take gradients,\n",
    "but raw autograd can be a bit too low-level for defining complex neural networks;\n",
    "this is where the nn package can help. The nn package defines a set of Modules,\n",
    "which you can think of as a neural network layer that has produces output from\n",
    "input and may have some trainable weights or other state.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/home/ansuini/repos/WellTemperedSGD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# After constructing the model we use the .to() method to move it to the\n",
    "# desired device.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        ).to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print parameters to text file\n",
    "\n",
    "\n",
    "Usually you do not want to do this, since there is a method to\n",
    "save the current state on a python dictionary.\n",
    "But if you want to do that I suggest to define a custom function\n",
    "that let you do that from this dictionary.\n",
    "In what follows I make a sketch of such a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.2907e-02, -1.1424e-02, -1.6427e-02,  ...,  2.5289e-02,\n",
      "         -1.1739e-02,  7.4289e-03],\n",
      "        [-2.6876e-02,  1.1051e-02,  1.5004e-02,  ...,  2.7995e-02,\n",
      "          7.2402e-03,  9.9765e-04],\n",
      "        [ 1.3872e-02,  3.8234e-03, -1.5813e-02,  ..., -1.0161e-03,\n",
      "         -1.0347e-02, -2.1643e-02],\n",
      "        ...,\n",
      "        [-2.5930e-02, -1.7957e-02, -2.8090e-02,  ...,  4.5477e-03,\n",
      "          1.7450e-02,  2.9173e-02],\n",
      "        [-6.5370e-03,  3.9465e-03,  2.0835e-02,  ...,  2.0754e-02,\n",
      "         -2.4976e-02, -3.0415e-03],\n",
      "        [-1.6758e-02,  2.9056e-02, -8.1800e-03,  ..., -2.5358e-02,\n",
      "          2.0175e-02,  5.2555e-03]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2625,  0.8135, -2.2165, -2.3456,  0.0207,  1.6068,  1.1609,\n",
      "        -3.0683, -1.3745, -2.9605,  0.2081,  2.5556, -1.6344,  0.5483,\n",
      "        -0.3025, -1.5084, -1.0039, -0.1315,  1.2407, -2.4769,  1.2136,\n",
      "         0.2585,  1.2187,  0.2941, -2.7129,  1.4293, -1.0003,  2.5029,\n",
      "         0.4624,  0.2666,  2.7708,  2.6269, -3.1050,  2.9464, -1.9948,\n",
      "        -1.2801, -1.8513,  2.9983, -1.7148, -1.8392,  2.7309,  1.0738,\n",
      "        -1.0425,  1.2977,  0.8670,  1.3709,  0.4026, -2.2067, -0.5036,\n",
      "        -1.1701,  1.9262,  1.4144,  2.8050, -2.8307,  1.2338, -1.6240,\n",
      "         0.3049,  2.3033,  2.3912,  1.5905, -1.3267, -2.7430,  0.6526,\n",
      "         1.4171, -0.3205,  1.5288, -2.4885,  2.7323, -1.6915, -2.8330,\n",
      "         1.2244,  2.4868,  2.7224,  2.6725, -2.4273,  0.3779,  1.2241,\n",
      "         0.6746,  2.5294,  1.2567,  1.3243,  2.4216,  2.2218, -2.5840,\n",
      "         1.1364, -2.4437, -1.9089,  0.6866, -1.7645, -2.0133,  0.0562,\n",
      "        -2.6351, -2.8605,  2.9547,  1.7296,  2.4267,  1.2139, -0.0091,\n",
      "        -0.6304,  2.2503])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[-2.8378, -3.2321, -5.5812,  1.2383, -9.7676,  0.3424,  7.2454,\n",
      "         -4.3772, -5.7181, -8.2322,  7.1318,  1.5756, -3.8421, -1.3229,\n",
      "         -0.6251, -7.9360, -5.3924,  4.2593,  4.3502, -3.5882, -5.0237,\n",
      "          1.1878,  5.3073, -7.1640, -9.4575, -5.8433, -2.5118,  5.5807,\n",
      "         -5.4541, -2.5350, -7.5007,  5.6919, -6.1560, -8.0494, -0.4197,\n",
      "         -1.0498,  7.7360,  9.5014,  3.9406, -6.5623,  4.1258, -4.2565,\n",
      "         -0.5148,  3.9961,  7.0562, -9.7346, -9.3929, -6.6381,  8.7139,\n",
      "         -6.6677,  3.0673,  6.6549,  2.5068,  0.5368,  8.5783,  7.0722,\n",
      "          8.8366, -3.6725,  2.3064, -0.9165, -3.0633,  4.1346, -8.1954,\n",
      "          7.5372,  5.5391,  7.4829,  3.9394,  9.8453,  7.6776,  3.8706,\n",
      "          1.2442,  7.4530, -3.0549, -8.2943, -1.6563,  6.9479, -9.9545,\n",
      "         -5.7079,  6.1813, -7.8062, -1.7176,  3.6498,  5.0117,  7.5035,\n",
      "          4.4396,  8.9681, -0.9040, -3.4566, -8.2919,  3.6655, -8.6947,\n",
      "         -9.1497, -5.8399,  1.6392, -9.9674,  5.0869, -2.1726,  4.1214,\n",
      "         -5.2166, -6.8295],\n",
      "        [ 1.3399, -6.9373,  3.2932, -9.8848, -9.6169,  1.4819, -8.8975,\n",
      "         -5.2920, -9.3061,  1.9011, -2.4133,  1.4669,  0.8965,  7.5150,\n",
      "          0.8627, -9.6556,  3.0910,  0.5030, -5.9227,  2.2511,  9.4688,\n",
      "         -2.1325,  8.2389, -5.7107,  8.6873,  4.1593,  9.4126, -0.2932,\n",
      "          4.4443,  5.9847, -0.5976, -1.8656, -0.7986, -8.2433,  0.1780,\n",
      "          6.4438,  7.1081,  0.7289, -1.9480, -7.5793,  6.2994, -7.6030,\n",
      "         -9.8968, -2.3485, -3.2840,  2.7149, -3.3444, -7.3169,  7.3874,\n",
      "         -3.7632, -1.5432, -9.1171, -2.0608, -4.5152,  6.3858, -0.5526,\n",
      "          1.6749, -5.7821, -9.5221,  7.0890,  5.0519,  6.2660, -8.5289,\n",
      "         -0.5311,  2.1210,  6.9532,  2.3130, -6.7070, -9.3164,  4.7475,\n",
      "         -3.0500,  7.4608, -6.7851,  1.3050, -8.4452,  9.5446,  9.4535,\n",
      "          8.9104,  1.8499, -4.8658,  5.9755,  5.9195, -1.9840,  0.7481,\n",
      "          2.8346,  8.5031, -7.1933,  3.8112, -0.7870,  2.8536,  4.3855,\n",
      "          8.7671, -8.5417,  9.7796, -4.1070, -8.6877,  5.1069, -1.7568,\n",
      "         -3.7386,  0.5095],\n",
      "        [-7.0487, -8.5047, -0.8033, -0.8687,  6.9001,  8.3297,  1.0350,\n",
      "          4.3684,  8.5466,  9.9624, -5.1889,  8.2598, -6.0254, -1.5916,\n",
      "         -1.9466, -4.1092, -9.7075,  9.8304,  5.2183, -2.9071, -7.7568,\n",
      "         -5.5299, -3.3663, -5.4985,  3.8090,  3.2542,  9.7183,  4.4607,\n",
      "         -3.9184, -5.9211, -9.1989,  5.0664,  0.9529,  3.4213, -7.8299,\n",
      "          9.8402,  4.0797,  3.2604,  9.3735, -1.3292,  6.2843,  2.6486,\n",
      "         -3.1136,  4.7584, -1.7703, -7.8521, -4.3234, -3.6403, -1.6168,\n",
      "         -5.0917,  9.7516,  8.0071, -8.6575,  6.8728,  1.2117,  5.4407,\n",
      "          3.2883,  2.3312, -5.4470,  6.3641, -1.4725,  7.7432, -4.1970,\n",
      "         -9.1002,  6.6876,  0.2227,  8.5050,  6.1223, -6.5009, -2.2642,\n",
      "          5.3184,  4.4651,  5.1090, -0.5346, -0.3498, -8.5406, -4.9191,\n",
      "          4.5203, -1.8175,  5.6374, -5.5855,  8.6946, -4.2915,  4.4480,\n",
      "          3.0975, -1.3267, -1.1288, -7.2239,  8.0388,  7.1305, -8.5572,\n",
      "          1.6989, -2.7949, -4.4028, -3.8171, -3.2676,  2.6532,  9.5652,\n",
      "         -3.6860, -6.7961],\n",
      "        [-9.8463, -6.7862,  3.0295,  5.9095,  6.7791, -3.3255,  4.1076,\n",
      "         -2.2052, -6.7874,  1.5985,  1.3104,  4.1556, -6.9977, -6.1516,\n",
      "          9.6488,  3.2508, -2.0580, -5.4539,  4.1933,  3.5908, -6.1151,\n",
      "         -0.3752,  2.0155,  0.1386,  1.6545,  7.3984,  3.9250, -2.6003,\n",
      "         -6.6164, -6.6784,  9.2996, -9.2340, -1.8399,  4.2596,  3.5766,\n",
      "         -5.2815,  6.0369, -3.2753, -3.0275, -2.5403,  6.9776,  3.8318,\n",
      "         -4.6241,  2.5822, -4.2296, -7.4017,  9.1724, -9.5759, -5.6558,\n",
      "          0.1208,  7.5781, -8.7575, -7.5795,  1.5732, -1.0781,  1.4992,\n",
      "          8.4580, -8.8482, -9.1961,  4.8305,  7.2866, -5.7104, -6.2072,\n",
      "          6.1802,  4.6006,  1.9840, -2.3831,  8.1197, -4.9844,  5.3703,\n",
      "         -5.8710,  3.3784, -4.2520,  2.8681, -8.0688, -8.4113, -0.4387,\n",
      "          5.1790, -7.7957,  3.4488,  7.3543, -7.2718, -9.2761, -7.0092,\n",
      "         -8.6700,  5.2916,  7.8127,  9.9233, -7.2410,  3.0846,  9.9979,\n",
      "          2.1473,  7.8056,  7.1629, -5.9595,  8.2167,  0.3996,  8.2050,\n",
      "         -3.7350, -3.5015],\n",
      "        [ 9.9454,  2.9970, -6.2681,  0.0788,  3.3245, -6.3143,  1.6524,\n",
      "         -1.2003,  7.8771, -7.8390,  6.9982,  1.0116,  8.8064, -3.2363,\n",
      "          5.3581, -1.8071, -3.8840,  0.0600, -0.2483, -4.0001, -3.9139,\n",
      "         -9.9041, -6.6201, -9.8985,  0.0364,  9.3524, -6.3436, -3.5210,\n",
      "          8.4787,  9.9836, -6.9281, -6.9008, -6.8841,  6.6498,  7.2871,\n",
      "          0.5625,  7.5114,  6.3590,  2.7237, -7.3282,  7.1195, -4.3561,\n",
      "         -8.0756,  4.5701,  4.3407, -0.7813,  3.5154, -8.3111, -4.5615,\n",
      "          7.4739,  9.1053, -5.2538, -7.0242,  7.4091,  4.8565,  7.8558,\n",
      "          8.9567, -8.3694,  3.0142, -5.5953, -1.7163, -7.2729, -8.0782,\n",
      "          7.1261, -4.0796,  9.2113,  0.1097, -9.4435, -9.7806, -3.2136,\n",
      "          7.5635, -0.3241,  0.6621, -9.8712, -1.3560,  1.5174, -5.1259,\n",
      "         -9.3013,  7.8977, -4.4576, -8.8366, -0.1447,  6.4990, -2.0406,\n",
      "         -4.8804,  8.7885,  2.8458,  6.6085,  6.4021, -8.6098,  2.0571,\n",
      "         -7.6328, -6.9570, -1.7198,  1.7504, -8.2002, -8.4931,  7.7398,\n",
      "         -5.9659,  5.9647],\n",
      "        [-6.6724, -1.9135,  9.3076,  1.4567, -0.1635, -7.9267, -6.8779,\n",
      "         -6.4028, -8.7021,  3.9012, -5.1245, -4.9214, -7.1789,  4.2069,\n",
      "         -1.8504,  2.4653, -9.8707, -0.0313, -2.3819,  2.9078,  2.8968,\n",
      "          5.1276,  5.4797,  2.3776, -9.4608, -5.5463, -0.2488,  7.8400,\n",
      "         -7.7574,  1.3578,  9.8332, -5.9463, -0.3115,  2.6425,  4.2946,\n",
      "         -5.4999,  1.7275,  0.9405, -7.7893,  3.6139,  9.6076,  0.5176,\n",
      "          2.7776, -7.3662,  1.2761, -7.3482, -0.3489,  6.9422,  0.7515,\n",
      "          2.7511, -9.4657,  9.7964,  3.0970, -1.8873, -3.8289, -9.6629,\n",
      "         -0.5120, -6.5626,  3.1694,  9.1508, -4.4368, -8.2836,  8.3432,\n",
      "         -9.3055, -1.3953,  5.1351, -2.7618, -5.6299, -1.5003,  4.8077,\n",
      "          3.8970, -1.5279,  1.5428,  1.0549, -0.3444, -6.2749, -6.7812,\n",
      "         -4.9326,  8.0022,  6.2769, -6.4524, -8.7321,  0.0331, -0.8808,\n",
      "          3.1917,  6.3194, -4.5410, -1.4507, -5.1048,  1.6025,  8.7922,\n",
      "         -7.0062,  9.1062,  9.4425, -5.6480, -5.4201, -7.6478, -1.7130,\n",
      "         -9.9624,  7.9651],\n",
      "        [ 9.2616,  1.4435, -1.0168, -7.3421,  6.9762,  0.2817,  0.8598,\n",
      "          4.5001,  8.3262, -3.2033, -5.4258, -2.5631, -4.0301,  6.8603,\n",
      "          1.6487, -8.0691, -8.1406, -8.9033, -0.5072, -9.6059,  1.1842,\n",
      "          6.5805,  4.9524,  5.9565,  4.2423,  8.6410,  5.8937, -3.0410,\n",
      "          3.0499,  2.8124,  0.4292, -8.4061,  9.8108,  3.4233,  4.4122,\n",
      "          6.0222, -9.0050,  9.7865, -9.0476, -1.4743,  5.4539, -8.6309,\n",
      "         -0.4487,  5.9466, -2.8473, -6.0484,  2.0357, -4.6415,  8.0507,\n",
      "         -8.3429, -2.2718, -4.1315, -3.5608,  9.9148,  1.1985, -7.8074,\n",
      "          9.4173,  5.8749, -6.0579,  6.6646, -9.7658, -1.6888,  1.3411,\n",
      "         -7.3241, -7.3622,  5.2026, -1.0511, -0.5583,  6.3975,  3.3905,\n",
      "         -7.7423, -6.1653,  9.7952,  8.6870,  6.2022,  3.8736, -3.2115,\n",
      "          1.1308,  8.9641,  9.5318, -4.0232, -7.4047,  2.9951, -2.9141,\n",
      "          8.9682, -5.0700, -8.2984,  5.3325,  3.4146, -8.1948, -9.9317,\n",
      "          6.3424,  5.1122,  6.5495, -6.0309, -1.2541,  2.5948, -3.2505,\n",
      "          1.5718,  5.4253],\n",
      "        [ 5.4165, -4.5617,  3.0674,  9.2553, -0.5463, -7.1234, -3.2871,\n",
      "         -1.0802, -8.9133, -1.4715, -2.8634, -1.4740,  4.4361, -1.8442,\n",
      "          0.9897,  3.2512,  5.9101,  8.6322, -8.4780,  0.1419,  4.8372,\n",
      "          0.5842, -2.6968,  9.6897,  3.6489,  2.2598,  1.0249,  8.8789,\n",
      "          6.6001,  1.7879, -7.9715, -6.6830,  2.6992,  2.1820, -2.5540,\n",
      "         -8.5619, -5.8887,  2.6745, -6.2751, -5.3937, -3.1940, -1.7843,\n",
      "         -9.3018,  6.8113, -6.0848,  7.9708, -5.6594, -3.0556, -9.2582,\n",
      "          4.2456, -8.1167, -6.7579, -0.6495,  9.3836, -2.0887, -2.3149,\n",
      "         -8.6693,  2.6612,  1.3554, -3.2069, -1.2277, -5.1053, -6.1734,\n",
      "         -9.5493,  6.5667, -4.8257, -1.7460,  7.9732, -7.1762, -3.7969,\n",
      "          2.7631, -7.0017, -5.1975,  8.0468,  0.0912,  5.6892, -7.3477,\n",
      "          9.2468,  4.1924,  3.3666,  7.8942,  8.1847, -5.5089, -3.0629,\n",
      "          6.0789, -4.8871,  6.9456, -0.5025, -1.6664,  0.2897,  3.1048,\n",
      "         -7.2474, -8.7693, -3.9782, -4.9330, -9.8148,  4.9449,  7.3142,\n",
      "         -4.3770,  4.8353],\n",
      "        [-4.7323,  0.4327,  8.4159, -9.8814,  2.1345, -3.1830, -2.2193,\n",
      "          7.7434, -7.4371,  1.5293, -8.0742, -4.4916, -8.7073,  6.3093,\n",
      "         -3.3973, -8.0884,  0.9208,  9.0413, -0.8903, -1.9468,  2.9410,\n",
      "          9.6484, -4.7880,  2.8934, -0.1288,  2.8209,  4.4601, -4.4146,\n",
      "         -3.6979, -0.6596, -4.5242, -4.2727,  7.1099,  4.5413,  4.7203,\n",
      "          0.0122,  0.8217,  9.6048, -9.8668,  3.2877, -1.2839, -5.3067,\n",
      "          1.7000, -0.9783,  4.4732, -3.2303,  8.0425,  2.2978,  7.2823,\n",
      "         -7.6544, -7.9615, -3.6308, -0.6994,  8.5162, -5.3371,  5.9191,\n",
      "          7.8699,  9.9295, -3.5859,  5.2737,  0.6230, -0.8103,  0.6104,\n",
      "         -7.0156, -9.6900, -4.5224,  5.1887,  0.9990,  9.5022, -9.0032,\n",
      "          4.8024,  4.2821,  7.7127,  9.6477,  1.2040, -4.3996, -6.2481,\n",
      "         -0.3613,  1.2396, -2.2577,  4.8727,  7.3883, -7.9478, -7.1721,\n",
      "         -9.6860, -0.2444, -0.9927,  1.1019, -4.7759,  1.7872,  9.9276,\n",
      "         -4.3421, -5.3416,  5.5536,  5.3075,  5.5716, -8.7704,  1.3508,\n",
      "          9.4629, -4.6115],\n",
      "        [ 2.3385,  4.4720,  1.4458,  4.5589,  6.1423,  4.7673,  9.2479,\n",
      "         -1.2776,  6.3874,  5.6277, -4.7574, -0.5155,  0.6941,  4.4573,\n",
      "          2.8106, -4.0620, -0.6146,  0.6310, -9.0036, -8.0215, -9.1548,\n",
      "          6.4575, -5.2439, -0.3553, -1.8433, -9.9184,  9.0226,  5.4521,\n",
      "          3.6925, -9.2144,  2.2371, -1.2166,  9.7121,  1.1168, -0.2320,\n",
      "         -2.2919, -0.4093, -6.7132, -3.8416, -8.0637, -9.4476, -2.7718,\n",
      "         -3.2085, -1.0811,  1.4129,  5.5369, -5.5445, -5.8078,  7.3843,\n",
      "          8.3210,  8.4802, -7.7198, -3.2212, -8.8324, -4.0634, -4.3560,\n",
      "          2.7376, -0.5703,  5.2845,  6.0372, -4.2383, -8.5115, -2.4911,\n",
      "         -1.6337,  1.5484, -4.2308, -8.5391,  6.5309,  8.6001, -9.0523,\n",
      "          1.1721, -1.2894, -3.6801, -3.4769, -9.9525,  5.0015, -0.6510,\n",
      "          2.2919, -5.0208, -4.2185, -1.0120,  7.9920, -5.0938,  1.8722,\n",
      "          3.1512,  3.1517,  6.7482,  7.9389,  5.2636, -9.7851,  1.9925,\n",
      "         -4.9827, -3.9371,  8.6566, -3.7995, -5.9944, -1.1347, -2.4392,\n",
      "         -3.0382, -8.7809]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [ 3.1094,  6.8844,  7.3908,  9.2589, -2.5502, -1.3608, -9.2861,\n",
      "         3.6324, -8.7962, -3.6111])\n"
     ]
    }
   ],
   "source": [
    "# the parameters are accessible from the generator model.parameters()\n",
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the current state\n",
    "\n",
    "# in this case torch.save_state_dict() did not work\n",
    "torch.save(model.state_dict(), ROOT + 'my_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load it\n",
    "state = torch.load(ROOT + 'my_model.pt')\n",
    "#print(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ROOT + 'my_model.txt'\n",
    "def printState2Txt(filename, state):\n",
    "    with open(filename,'w') as f:\n",
    "        for d in state.keys():\n",
    "            f.write(d + '\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "            x = state[d]\n",
    "            sx = x.size()\n",
    "\n",
    "            if len(sx) == 1:\n",
    "                for i in range(x.size(0)):                    \n",
    "                    f.write(str(x[i].item()) + ' ')\n",
    "                f.write('\\n')\n",
    "\n",
    "            if len(sx) == 2:\n",
    "                for i in range(x.size(0)):\n",
    "                    for j in range(x.size(1)):\n",
    "                        f.write(str(x[i,j].item()) + ' ')\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "printState2Txt(filename,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 625.9913330078125\n",
      "1 579.6355590820312\n",
      "2 539.7922973632812\n",
      "3 504.93304443359375\n",
      "4 473.69317626953125\n",
      "5 445.5487976074219\n",
      "6 419.7396240234375\n",
      "7 395.89495849609375\n",
      "8 373.8634338378906\n",
      "9 353.36865234375\n",
      "10 334.2891845703125\n",
      "11 316.36187744140625\n",
      "12 299.6120300292969\n",
      "13 283.8233642578125\n",
      "14 268.7804870605469\n",
      "15 254.46823120117188\n",
      "16 240.87664794921875\n",
      "17 227.9961395263672\n",
      "18 215.75332641601562\n",
      "19 204.11007690429688\n",
      "20 193.05709838867188\n",
      "21 182.54612731933594\n",
      "22 172.54734802246094\n",
      "23 163.026611328125\n",
      "24 153.9678955078125\n",
      "25 145.37112426757812\n",
      "26 137.21580505371094\n",
      "27 129.48358154296875\n",
      "28 122.16249084472656\n",
      "29 115.22429656982422\n",
      "30 108.65581512451172\n",
      "31 102.43675231933594\n",
      "32 96.5379638671875\n",
      "33 90.97488403320312\n",
      "34 85.6992416381836\n",
      "35 80.72434997558594\n",
      "36 76.01497650146484\n",
      "37 71.57112121582031\n",
      "38 67.39192962646484\n",
      "39 63.45368576049805\n",
      "40 59.75044631958008\n",
      "41 56.26570510864258\n",
      "42 52.986900329589844\n",
      "43 49.90156173706055\n",
      "44 47.003631591796875\n",
      "45 44.28755569458008\n",
      "46 41.732177734375\n",
      "47 39.32762908935547\n",
      "48 37.065006256103516\n",
      "49 34.94131851196289\n",
      "50 32.93827819824219\n",
      "51 31.057519912719727\n",
      "52 29.291988372802734\n",
      "53 27.633880615234375\n",
      "54 26.079206466674805\n",
      "55 24.615755081176758\n",
      "56 23.23928451538086\n",
      "57 21.94703483581543\n",
      "58 20.73316192626953\n",
      "59 19.58991050720215\n",
      "60 18.51479148864746\n",
      "61 17.501888275146484\n",
      "62 16.54766082763672\n",
      "63 15.651220321655273\n",
      "64 14.80754566192627\n",
      "65 14.012666702270508\n",
      "66 13.26541519165039\n",
      "67 12.564332962036133\n",
      "68 11.903532028198242\n",
      "69 11.2799654006958\n",
      "70 10.691065788269043\n",
      "71 10.134954452514648\n",
      "72 9.61047077178955\n",
      "73 9.115609169006348\n",
      "74 8.648877143859863\n",
      "75 8.208019256591797\n",
      "76 7.791189670562744\n",
      "77 7.3976287841796875\n",
      "78 7.025817394256592\n",
      "79 6.674678802490234\n",
      "80 6.3427815437316895\n",
      "81 6.028321266174316\n",
      "82 5.730713367462158\n",
      "83 5.448814868927002\n",
      "84 5.182754993438721\n",
      "85 4.931509971618652\n",
      "86 4.693474292755127\n",
      "87 4.467715263366699\n",
      "88 4.25377893447876\n",
      "89 4.051303386688232\n",
      "90 3.8589720726013184\n",
      "91 3.676478147506714\n",
      "92 3.503406524658203\n",
      "93 3.339463472366333\n",
      "94 3.1839964389801025\n",
      "95 3.0362930297851562\n",
      "96 2.8959341049194336\n",
      "97 2.7625744342803955\n",
      "98 2.636033296585083\n",
      "99 2.5155715942382812\n",
      "100 2.4011290073394775\n",
      "101 2.2924611568450928\n",
      "102 2.1891183853149414\n",
      "103 2.090625762939453\n",
      "104 1.9970812797546387\n",
      "105 1.9080642461776733\n",
      "106 1.8234621286392212\n",
      "107 1.742867112159729\n",
      "108 1.6661454439163208\n",
      "109 1.5929094552993774\n",
      "110 1.5225704908370972\n",
      "111 1.4556021690368652\n",
      "112 1.3917193412780762\n",
      "113 1.3308513164520264\n",
      "114 1.2729105949401855\n",
      "115 1.2176764011383057\n",
      "116 1.164987564086914\n",
      "117 1.114645004272461\n",
      "118 1.066643476486206\n",
      "119 1.0208311080932617\n",
      "120 0.9771061539649963\n",
      "121 0.9353992342948914\n",
      "122 0.8955477476119995\n",
      "123 0.8575102090835571\n",
      "124 0.8211673498153687\n",
      "125 0.7864810824394226\n",
      "126 0.7533639073371887\n",
      "127 0.7217448949813843\n",
      "128 0.6915194988250732\n",
      "129 0.6626624464988708\n",
      "130 0.6351099610328674\n",
      "131 0.6087682247161865\n",
      "132 0.5835716128349304\n",
      "133 0.5596531629562378\n",
      "134 0.5367746353149414\n",
      "135 0.5149052739143372\n",
      "136 0.4940064549446106\n",
      "137 0.4740154445171356\n",
      "138 0.45487451553344727\n",
      "139 0.4365634620189667\n",
      "140 0.4190305769443512\n",
      "141 0.4022495150566101\n",
      "142 0.38618630170822144\n",
      "143 0.37079957127571106\n",
      "144 0.35607632994651794\n",
      "145 0.34199103713035583\n",
      "146 0.3284851610660553\n",
      "147 0.3155438005924225\n",
      "148 0.3031466007232666\n",
      "149 0.29127541184425354\n",
      "150 0.2798958718776703\n",
      "151 0.26898008584976196\n",
      "152 0.2585318386554718\n",
      "153 0.248505100607872\n",
      "154 0.2388884425163269\n",
      "155 0.22966912388801575\n",
      "156 0.22083072364330292\n",
      "157 0.2123609185218811\n",
      "158 0.20423410832881927\n",
      "159 0.19644567370414734\n",
      "160 0.18896466493606567\n",
      "161 0.1817941516637802\n",
      "162 0.17490839958190918\n",
      "163 0.16829626262187958\n",
      "164 0.1619507372379303\n",
      "165 0.15585735440254211\n",
      "166 0.1500081568956375\n",
      "167 0.14439629018306732\n",
      "168 0.13900509476661682\n",
      "169 0.1338263750076294\n",
      "170 0.12885244190692902\n",
      "171 0.12407726049423218\n",
      "172 0.11948984116315842\n",
      "173 0.11508588492870331\n",
      "174 0.11085311323404312\n",
      "175 0.10679667443037033\n",
      "176 0.10289919376373291\n",
      "177 0.09916035085916519\n",
      "178 0.09556584805250168\n",
      "179 0.09210900962352753\n",
      "180 0.08878745883703232\n",
      "181 0.08559364080429077\n",
      "182 0.08252211660146713\n",
      "183 0.07956678420305252\n",
      "184 0.0767243281006813\n",
      "185 0.07399117946624756\n",
      "186 0.07135863602161407\n",
      "187 0.06882624328136444\n",
      "188 0.06638878583908081\n",
      "189 0.06404294073581696\n",
      "190 0.06178778409957886\n",
      "191 0.05961436778306961\n",
      "192 0.05751959607005119\n",
      "193 0.05550454929471016\n",
      "194 0.05356520786881447\n",
      "195 0.0516977496445179\n",
      "196 0.049897901713848114\n",
      "197 0.04816539213061333\n",
      "198 0.046495042741298676\n",
      "199 0.044886182993650436\n",
      "200 0.043337129056453705\n",
      "201 0.04184260219335556\n",
      "202 0.040403325110673904\n",
      "203 0.039016690105199814\n",
      "204 0.03767964988946915\n",
      "205 0.03639129176735878\n",
      "206 0.03514920547604561\n",
      "207 0.03395141288638115\n",
      "208 0.03279663994908333\n",
      "209 0.03168448060750961\n",
      "210 0.03061099350452423\n",
      "211 0.02957579866051674\n",
      "212 0.02857786789536476\n",
      "213 0.027615070343017578\n",
      "214 0.026686454191803932\n",
      "215 0.025790708139538765\n",
      "216 0.024926139041781425\n",
      "217 0.024091875180602074\n",
      "218 0.023286692798137665\n",
      "219 0.02251000516116619\n",
      "220 0.02176067605614662\n",
      "221 0.021037716418504715\n",
      "222 0.020339656621217728\n",
      "223 0.019666051492094994\n",
      "224 0.01901523768901825\n",
      "225 0.01838698238134384\n",
      "226 0.017780620604753494\n",
      "227 0.017195094376802444\n",
      "228 0.016629591584205627\n",
      "229 0.016084082424640656\n",
      "230 0.015556978061795235\n",
      "231 0.015047810040414333\n",
      "232 0.014555754140019417\n",
      "233 0.014080557972192764\n",
      "234 0.013621574267745018\n",
      "235 0.013178138062357903\n",
      "236 0.012749698013067245\n",
      "237 0.01233613956719637\n",
      "238 0.011936393566429615\n",
      "239 0.0115499347448349\n",
      "240 0.011176801286637783\n",
      "241 0.010816059075295925\n",
      "242 0.010467319749295712\n",
      "243 0.010130389593541622\n",
      "244 0.009804639033973217\n",
      "245 0.009489763528108597\n",
      "246 0.009185416623950005\n",
      "247 0.008891385048627853\n",
      "248 0.008606978692114353\n",
      "249 0.008331982418894768\n",
      "250 0.008066046983003616\n",
      "251 0.007808872498571873\n",
      "252 0.007560465019196272\n",
      "253 0.007320139557123184\n",
      "254 0.0070876628160476685\n",
      "255 0.006862824782729149\n",
      "256 0.006645341869443655\n",
      "257 0.006434974726289511\n",
      "258 0.0062315319664776325\n",
      "259 0.0060347518883645535\n",
      "260 0.005844415631145239\n",
      "261 0.005660225637257099\n",
      "262 0.00548198539763689\n",
      "263 0.0053095705807209015\n",
      "264 0.0051427618600428104\n",
      "265 0.004981421399861574\n",
      "266 0.004825257696211338\n",
      "267 0.004674138035625219\n",
      "268 0.004527910612523556\n",
      "269 0.004386372398585081\n",
      "270 0.004249465651810169\n",
      "271 0.00411689467728138\n",
      "272 0.0039885989390313625\n",
      "273 0.003864394500851631\n",
      "274 0.0037442229222506285\n",
      "275 0.0036278374027460814\n",
      "276 0.00351514364592731\n",
      "277 0.0034061504993587732\n",
      "278 0.0033006002195179462\n",
      "279 0.003198441583663225\n",
      "280 0.0030995027627795935\n",
      "281 0.0030037485994398594\n",
      "282 0.002910956973209977\n",
      "283 0.002821145812049508\n",
      "284 0.0027342152316123247\n",
      "285 0.002649940550327301\n",
      "286 0.002568433526903391\n",
      "287 0.002489430131390691\n",
      "288 0.0024129513185471296\n",
      "289 0.002338858786970377\n",
      "290 0.0022670873440802097\n",
      "291 0.0021975787822157145\n",
      "292 0.0021302527748048306\n",
      "293 0.0020650257356464863\n",
      "294 0.002001864369958639\n",
      "295 0.0019406948704272509\n",
      "296 0.00188143877312541\n",
      "297 0.0018240056233480573\n",
      "298 0.001768364803865552\n",
      "299 0.0017145071178674698\n",
      "300 0.001662306021898985\n",
      "301 0.001611717976629734\n",
      "302 0.0015627165557816625\n",
      "303 0.001515230629593134\n",
      "304 0.0014692293480038643\n",
      "305 0.0014246365753933787\n",
      "306 0.0013814725680276752\n",
      "307 0.0013396015856415033\n",
      "308 0.0012990132672712207\n",
      "309 0.0012597180902957916\n",
      "310 0.0012216457398608327\n",
      "311 0.0011847543064504862\n",
      "312 0.0011489486787468195\n",
      "313 0.0011142697185277939\n",
      "314 0.0010806515347212553\n",
      "315 0.0010480734053999186\n",
      "316 0.0010164924897253513\n",
      "317 0.0009858958655968308\n",
      "318 0.000956222356762737\n",
      "319 0.0009275107877328992\n",
      "320 0.000899708189535886\n",
      "321 0.0008727504755370319\n",
      "322 0.0008466355502605438\n",
      "323 0.0008213118999265134\n",
      "324 0.0007967593264766037\n",
      "325 0.0007729632779955864\n",
      "326 0.0007498790509998798\n",
      "327 0.0007275029201991856\n",
      "328 0.0007058278424665332\n",
      "329 0.0006847833865322173\n",
      "330 0.0006643898668698967\n",
      "331 0.0006446174229495227\n",
      "332 0.0006254389882087708\n",
      "333 0.0006068488000892103\n",
      "334 0.0005888118757866323\n",
      "335 0.0005713367136195302\n",
      "336 0.0005543857114389539\n",
      "337 0.0005379304639063776\n",
      "338 0.000521986628882587\n",
      "339 0.0005065201548859477\n",
      "340 0.0004915145109407604\n",
      "341 0.0004769647784996778\n",
      "342 0.0004628550086636096\n",
      "343 0.0004491669242270291\n",
      "344 0.00043588984408415854\n",
      "345 0.0004230134072713554\n",
      "346 0.00041051956941373646\n",
      "347 0.00039840154931880534\n",
      "348 0.0003866579500027001\n",
      "349 0.0003752494521904737\n",
      "350 0.0003641952353063971\n",
      "351 0.00035345766809768975\n",
      "352 0.0003430549695622176\n",
      "353 0.00033296807669103146\n",
      "354 0.00032316683791577816\n",
      "355 0.0003136676677968353\n",
      "356 0.00030444131698459387\n",
      "357 0.0002955141826532781\n",
      "358 0.00028683198615908623\n",
      "359 0.0002784136449918151\n",
      "360 0.00027024492737837136\n",
      "361 0.0002623191103339195\n",
      "362 0.00025462638586759567\n",
      "363 0.00024716893676668406\n",
      "364 0.000239929897361435\n",
      "365 0.00023290184617508203\n",
      "366 0.0002260831097373739\n",
      "367 0.00021946888591628522\n",
      "368 0.0002130519860656932\n",
      "369 0.00020682437752839178\n",
      "370 0.00020078712259419262\n",
      "371 0.00019491673447191715\n",
      "372 0.0001892287691589445\n",
      "373 0.000183702795766294\n",
      "374 0.0001783488696673885\n",
      "375 0.00017314249998889863\n",
      "376 0.00016810056695248932\n",
      "377 0.00016319469432346523\n",
      "378 0.00015843888104427606\n",
      "379 0.00015382682613562793\n",
      "380 0.00014935832587070763\n",
      "381 0.00014500513498205692\n",
      "382 0.0001407844974892214\n",
      "383 0.00013668798783328384\n",
      "384 0.0001327179925283417\n",
      "385 0.00012885536125395447\n",
      "386 0.0001251103385584429\n",
      "387 0.0001214798103319481\n",
      "388 0.00011795415775850415\n",
      "389 0.00011453093611635268\n",
      "390 0.00011120962153654546\n",
      "391 0.00010797989671118557\n",
      "392 0.00010485199891263619\n",
      "393 0.00010181614925386384\n",
      "394 9.886106272460893e-05\n",
      "395 9.599684562999755e-05\n",
      "396 9.321806282969192e-05\n",
      "397 9.051714732777327e-05\n",
      "398 8.790081483311951e-05\n",
      "399 8.534928201697767e-05\n",
      "400 8.28879201435484e-05\n",
      "401 8.048766176216304e-05\n",
      "402 7.816106517566368e-05\n",
      "403 7.590136374346912e-05\n",
      "404 7.371141691692173e-05\n",
      "405 7.158109656302258e-05\n",
      "406 6.951281829969957e-05\n",
      "407 6.750389002263546e-05\n",
      "408 6.555797881446779e-05\n",
      "409 6.366395973600447e-05\n",
      "410 6.183140794746578e-05\n",
      "411 6.004459282848984e-05\n",
      "412 5.8315679780207574e-05\n",
      "413 5.6634667998878285e-05\n",
      "414 5.500418410520069e-05\n",
      "415 5.341687210602686e-05\n",
      "416 5.187695205677301e-05\n",
      "417 5.0386392103973776e-05\n",
      "418 4.893754521617666e-05\n",
      "419 4.752708991873078e-05\n",
      "420 4.615721627487801e-05\n",
      "421 4.483097291085869e-05\n",
      "422 4.3543364881770685e-05\n",
      "423 4.229073238093406e-05\n",
      "424 4.107420681975782e-05\n",
      "425 3.989441029261798e-05\n",
      "426 3.8749651139369234e-05\n",
      "427 3.763461427297443e-05\n",
      "428 3.655476757558063e-05\n",
      "429 3.5506076528690755e-05\n",
      "430 3.448692586971447e-05\n",
      "431 3.3493914088467136e-05\n",
      "432 3.253600516472943e-05\n",
      "433 3.160295818815939e-05\n",
      "434 3.069629019591957e-05\n",
      "435 2.981402212753892e-05\n",
      "436 2.896017576858867e-05\n",
      "437 2.812948150676675e-05\n",
      "438 2.732422944973223e-05\n",
      "439 2.654132003954146e-05\n",
      "440 2.578284329501912e-05\n",
      "441 2.5042199922609143e-05\n",
      "442 2.4326764105353504e-05\n",
      "443 2.3628275812370703e-05\n",
      "444 2.2953277948545292e-05\n",
      "445 2.2297446776065044e-05\n",
      "446 2.165990008506924e-05\n",
      "447 2.10385969694471e-05\n",
      "448 2.0437009879969992e-05\n",
      "449 1.9854689526255243e-05\n",
      "450 1.9286550013930537e-05\n",
      "451 1.873519431683235e-05\n",
      "452 1.8200360500486568e-05\n",
      "453 1.7679383745417e-05\n",
      "454 1.7175605535157956e-05\n",
      "455 1.6683698049746454e-05\n",
      "456 1.6209289242397062e-05\n",
      "457 1.574583802721463e-05\n",
      "458 1.52962365973508e-05\n",
      "459 1.4861393538012635e-05\n",
      "460 1.4436611309065484e-05\n",
      "461 1.4024969459569547e-05\n",
      "462 1.362445891572861e-05\n",
      "463 1.323626929661259e-05\n",
      "464 1.2859442904300522e-05\n",
      "465 1.2492560017562937e-05\n",
      "466 1.2136048098909669e-05\n",
      "467 1.1791509678005241e-05\n",
      "468 1.1454827472334728e-05\n",
      "469 1.1129089216410648e-05\n",
      "470 1.0813538210641127e-05\n",
      "471 1.0505113095859997e-05\n",
      "472 1.0204007594438735e-05\n",
      "473 9.915349437505938e-06\n",
      "474 9.632536603021435e-06\n",
      "475 9.358108400192577e-06\n",
      "476 9.091870197153185e-06\n",
      "477 8.833904757921118e-06\n",
      "478 8.583141607232392e-06\n",
      "479 8.339138730661944e-06\n",
      "480 8.101892490230966e-06\n",
      "481 7.87209501140751e-06\n",
      "482 7.647852726222482e-06\n",
      "483 7.4317340477136895e-06\n",
      "484 7.220329734991537e-06\n",
      "485 7.014076345512876e-06\n",
      "486 6.815470896981424e-06\n",
      "487 6.621628472203156e-06\n",
      "488 6.434062925109174e-06\n",
      "489 6.2512663134839386e-06\n",
      "490 6.0735828810720704e-06\n",
      "491 5.901340500713559e-06\n",
      "492 5.734122623834992e-06\n",
      "493 5.5719287956890184e-06\n",
      "494 5.4135102800501045e-06\n",
      "495 5.260696070763515e-06\n",
      "496 5.111560312798247e-06\n",
      "497 4.9665168262436055e-06\n",
      "498 4.825423729926115e-06\n",
      "499 4.688453827839112e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "  # override the __call__ operator so you can call them like functions. When\n",
    "  # doing so you pass a Tensor of input data to the Module and it produces\n",
    "  # a Tensor of output data.\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "  # values of y, and the loss function returns a Tensor containing the loss.\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  print(t, loss.item())\n",
    "  \n",
    "  # Zero the gradients before running the backward pass.\n",
    "  model.zero_grad()\n",
    "\n",
    "  # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "  # parameters of the model. Internally, the parameters of each Module are stored\n",
    "  # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "  # all learnable parameters in the model.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "  # we can access its data and gradients like we did before.\n",
    "  with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "      param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 0.4.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
